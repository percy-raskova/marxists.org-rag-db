# pytest configuration for MIA RAG System
# Optimized for TDD with 6 parallel instances

[pytest]
# Minimum pytest version
minversion = 6.0

# Test discovery paths
testpaths = tests

# Test file patterns
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Command line options (always applied)
# Note: INI format does NOT support inline comments after values
addopts =
    -ra
    --strict-markers
    --strict-config
    --tb=short
    --maxfail=5
    -vv
    --color=yes
    --cov=src/mia_rag
    --cov-branch
    --cov-report=term-missing
    --cov-report=html
    --cov-report=xml
    --cov-fail-under=80
    --durations=10
    --timeout=300
    --junit-xml=test-results.xml
    --html=test-report.html
    --self-contained-html

# Test markers for organization
markers =
    # Instance markers
    instance1: Tests for Storage & Pipeline (Instance 1)
    instance2: Tests for Embeddings (Instance 2)
    instance3: Tests for Weaviate Vector DB (Instance 3)
    instance4: Tests for Query & API (Instance 4)
    instance5: Tests for MCP Integration (Instance 5)
    instance6: Tests for Monitoring & Testing (Instance 6)

    # Test type markers
    unit: Unit tests (fast, isolated)
    integration: Integration tests (cross-module)
    contract: Contract tests (interface validation)
    scale: Scale tests (slow, requires resources)
    e2e: End-to-end tests (full pipeline)

    # Special markers
    slow: Slow running tests (>5 seconds)
    ai_generated: Test generated by AI (needs review)
    requires_gpu: Test requires GPU resources
    requires_gcp: Test requires GCP credentials
    requires_runpod: Test requires Runpod API
    requires_weaviate: Test requires Weaviate instance
    requires_redis: Test requires Redis instance

    # Development markers
    wip: Work in progress (skip in CI)
    flaky: Known flaky test (retry automatically)
    critical: Critical test (must pass)
    benchmark: Performance benchmark test

    # Example usage:
    # @pytest.mark.instance2
    # @pytest.mark.unit
    # @pytest.mark.requires_gpu
    # def test_embedding_generation():
    #     pass

# Asyncio configuration
asyncio_mode = auto

# Timeout configuration
timeout = 300
timeout_method = thread

# Strict mode
xfail_strict = true

# Logging
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] [%(name)s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests/test.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] [%(name)s] %(filename)s:%(lineno)d - %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Capture settings
capture = no                # Show print statements
console_output_style = progress

# Parallel execution (with pytest-xdist)
# Uncomment to enable parallel tests
# addopts = -n auto

# Instance-specific test collection examples:
# Run only Instance 2 tests:       pytest -m instance2
# Run only unit tests:             pytest -m unit
# Run Instance 2 unit tests:       pytest -m "instance2 and unit"
# Run integration tests:           pytest -m integration
# Skip slow tests:                 pytest -m "not slow"
# Run critical tests only:         pytest -m critical

# Environment variables for tests
[pytest:env]
ENVIRONMENT = testing
LOG_LEVEL = DEBUG
TESTING = true

# Ignore patterns
norecursedirs =
    .git
    .venv
    venv
    __pycache__
    *.egg-info
    .pytest_cache
    .mypy_cache
    .ruff_cache
    htmlcov
    node_modules
    .terraform
    work-logs
    build
    dist

# Doctest configuration
doctest_optionflags =
    NORMALIZE_WHITESPACE
    IGNORE_EXCEPTION_DETAIL
    ELLIPSIS

# Warning filters
filterwarnings =
    # Ignore specific warnings
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::ResourceWarning
    # Treat these as errors
    error::UserWarning:mia_rag.*

# Coverage configuration
[coverage:run]
source = src/mia_rag
branch = true
parallel = true
omit =
    */tests/*
    */__init__.py
    */conftest.py
    */test_*.py

[coverage:report]
exclude_lines =
    # Standard pragmas
    pragma: no cover

    # Don't complain about debug code
    def __repr__
    def __str__

    # Don't complain about defensive programming
    raise AssertionError
    raise NotImplementedError

    # Don't complain about non-runnable code
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    if typing.TYPE_CHECKING:

    # Don't complain about abstract methods
    @abstractmethod
    @abc.abstractmethod

    # Don't complain about property deletes
    @property\s*\n\s+def\s+\w+\(.*\):\s*\n\s+del\s

precision = 2
show_missing = true
skip_covered = false
skip_empty = false

# HTML coverage report
[coverage:html]
directory = htmlcov
title = MIA RAG Coverage Report
show_contexts = true

# XML coverage report (for CI)
[coverage:xml]
output = coverage.xml

# Plugin configuration
[tool:pytest]
plugins =
    pytest-cov
    pytest-asyncio
    pytest-mock
    pytest-xdist
    pytest-timeout
    pytest-benchmark
    pytest-html