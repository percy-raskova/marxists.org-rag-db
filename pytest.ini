# pytest configuration for MIA RAG System
# Optimized for TDD with 6 parallel instances

[pytest]
# Minimum pytest version
minversion = 6.0

# Test discovery paths
testpaths = tests

# Test file patterns
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Command line options (always applied)
addopts =
    # Reporting options
    -ra                          # Show all test outcomes
    --strict-markers             # Undefined markers are errors
    --strict-config              # Invalid config is an error
    --tb=short                   # Shorter tracebacks for readability
    --maxfail=5                  # Stop after 5 failures
    -vv                          # Very verbose output
    --color=yes                  # Colored output

    # Coverage options
    --cov=src/mia_rag           # Measure coverage for src
    --cov-branch                 # Branch coverage
    --cov-report=term-missing   # Show missing lines in terminal
    --cov-report=html           # Generate HTML report
    --cov-report=xml            # Generate XML for CI
    --cov-fail-under=80         # Fail if coverage < 80%

    # Performance
    --durations=10              # Show 10 slowest tests
    --timeout=300               # 5-minute timeout per test

    # Output
    --junit-xml=test-results.xml  # JUnit format for CI
    --html=test-report.html     # HTML report
    --self-contained-html       # Single HTML file

# Test markers for organization
markers =
    # Instance markers
    instance1: Tests for Storage & Pipeline (Instance 1)
    instance2: Tests for Embeddings (Instance 2)
    instance3: Tests for Weaviate Vector DB (Instance 3)
    instance4: Tests for Query & API (Instance 4)
    instance5: Tests for MCP Integration (Instance 5)
    instance6: Tests for Monitoring & Testing (Instance 6)

    # Test type markers
    unit: Unit tests (fast, isolated)
    integration: Integration tests (cross-module)
    contract: Contract tests (interface validation)
    scale: Scale tests (slow, requires resources)
    e2e: End-to-end tests (full pipeline)

    # Special markers
    slow: Slow running tests (>5 seconds)
    ai_generated: Test generated by AI (needs review)
    requires_gpu: Test requires GPU resources
    requires_gcp: Test requires GCP credentials
    requires_runpod: Test requires Runpod API
    requires_weaviate: Test requires Weaviate instance
    requires_redis: Test requires Redis instance

    # Development markers
    wip: Work in progress (skip in CI)
    flaky: Known flaky test (retry automatically)
    critical: Critical test (must pass)
    benchmark: Performance benchmark test

    # Example usage:
    # @pytest.mark.instance2
    # @pytest.mark.unit
    # @pytest.mark.requires_gpu
    # def test_embedding_generation():
    #     pass

# Asyncio configuration
asyncio_mode = auto

# Timeout configuration
timeout = 300               # Default 5-minute timeout
timeout_method = thread     # Use thread-based timeout

# Strict mode
xfail_strict = true        # XFAIL tests must actually fail

# Logging
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] [%(name)s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests/test.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] [%(name)s] %(filename)s:%(lineno)d - %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Capture settings
capture = no                # Show print statements
console_output_style = progress

# Parallel execution (with pytest-xdist)
# Uncomment to enable parallel tests
# addopts = -n auto

# Instance-specific test collection examples:
# Run only Instance 2 tests:       pytest -m instance2
# Run only unit tests:             pytest -m unit
# Run Instance 2 unit tests:       pytest -m "instance2 and unit"
# Run integration tests:           pytest -m integration
# Skip slow tests:                 pytest -m "not slow"
# Run critical tests only:         pytest -m critical

# Environment variables for tests
[pytest:env]
ENVIRONMENT = testing
LOG_LEVEL = DEBUG
TESTING = true

# Ignore patterns
norecursedirs =
    .git
    .venv
    venv
    __pycache__
    *.egg-info
    .pytest_cache
    .mypy_cache
    .ruff_cache
    htmlcov
    node_modules
    .terraform
    work-logs
    build
    dist

# Doctest configuration
doctest_optionflags =
    NORMALIZE_WHITESPACE
    IGNORE_EXCEPTION_DETAIL
    ELLIPSIS

# Warning filters
filterwarnings =
    # Ignore specific warnings
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::ResourceWarning
    # Treat these as errors
    error::UserWarning:mia_rag.*

# Coverage configuration
[coverage:run]
source = src/mia_rag
branch = true
parallel = true
omit =
    */tests/*
    */__init__.py
    */conftest.py
    */test_*.py

[coverage:report]
exclude_lines =
    # Standard pragmas
    pragma: no cover

    # Don't complain about debug code
    def __repr__
    def __str__

    # Don't complain about defensive programming
    raise AssertionError
    raise NotImplementedError

    # Don't complain about non-runnable code
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    if typing.TYPE_CHECKING:

    # Don't complain about abstract methods
    @abstractmethod
    @abc.abstractmethod

    # Don't complain about property deletes
    @property\s*\n\s+def\s+\w+\(.*\):\s*\n\s+del\s

precision = 2
show_missing = true
skip_covered = false
skip_empty = false

# HTML coverage report
[coverage:html]
directory = htmlcov
title = MIA RAG Coverage Report
show_contexts = true

# XML coverage report (for CI)
[coverage:xml]
output = coverage.xml

# Plugin configuration
[tool:pytest]
plugins =
    pytest-cov
    pytest-asyncio
    pytest-mock
    pytest-xdist
    pytest-timeout
    pytest-benchmark
    pytest-html